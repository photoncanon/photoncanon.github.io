<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0031)-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="keywords" content="Zhe Wang, 王哲, Electronic Engineering, The Chinese University of Hong Kong, Zhejiang University">
<meta name="description" content="ZWANG">
<link rel="stylesheet" href="./welcome_files/jemdoc.css" type="text/css">
<link rel="shortcut icon" href="./welcome_files/zwang2018.jpg" width="263px" height="300px">
<title>Wang Zhe</title>
<style type="text/css"></style>


<script type="text/javascript" src="./welcome_files/jquery.min.js"></script></head>
<body>
<div id="layout-content">
<p>


<script type="text/javascript" async="" src="./welcome_files/map.js"></script><script type="text/javascript">
<!--
    function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>
</p>

<table class="imgtable"><tbody><tr>
<td width="600" align="left">
<div id="toptitle"> 
  <h1><a href="http://www.ee.cuhk.edu.hk/~zwang/welcome.html">Wang Zhe</a> &nbsp; 王哲</h1>
</div>
<p>

Email: <a href="mailto:wangzhe@sensetime.com">wangzhe at sensetime dot com</a><br><br>
Address: Room 226, Core Building No.2, Hong Kong Science Park. <br>
Shatin, New Territory, Hong Kong <br><br>

<a href="https://scholar.google.com.hk/citations?user=546GPMoAAAAJ&hl=zh-CN">Google Scholar</a> &middot <a href="https://www.linkedin.com/home?trk=nav_responsive_tab_home">LinkedIn</a> &middot <a href="./welcome_files/cv/resume_cv.pdf">My resume</a> (last updated at 2020.9)<br>
<p></p>
</td>
<td align="left">
<img src="./welcome_files/zwang2018.jpg" alt="" width="360px" height="270px"> &nbsp;</td>

</tr>
</tbody></table>

<h2>  Short Bio </h2>
<ul>
<li><p>
    Currently I am a Director in the Intelligent Automative Group at SenseTime. Our 3D perception team is responsible for developing accurate and reliable perception systems for autonomous driving. 
    We are mainly focusing on Lidar based perception, camera-based general object perception and sensor fusion. We are also interested in various challenges in autonomous driving, such as mapping and localization and monocular/stereo/multi-view 3D object detection.  </p></li>

<li><p>
    I obtained my Ph.D. degree from EE department of CUHK in 2017, supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/"> Professor Xiaogang Wang </a>. I was both in <a href="http://http://ivp.ee.cuhk.edu.hk//">IVP lab</a> and <a href="http://mmlab.ie.cuhk.edu.hk/"> Multimedia Lab</a>. 
</p></li>
<li><p>
    Before I came to CUHK, I received a Bachelor's degree from <a href="http://en.sist.ustc.edu.cn/Departments/201105/t20110520_111931.html"> the department of Optical Engineering </a> of <a href="http://en.ustc.edu.cn/"> Zhejiang University (ZJU) </a> in July 2012. 
</p></li>
<li><p>  
  
   <font color="red">  I am now recuiting self-motivated interns / full-time researchers or software engineers with strong machine learning background and programming skills. If you are interested, please send your CV to my Email (wangzhe@sensetime.com) </font></p></li>

</ul>





<h2>  News </h2>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> I gave a talk on <a href="https://course.zhidx.com/c/Mjk3NTYyNDE1MWQxODdjNmUzOTM=">智东西公开课</a>. I mainly shared some recent progress of our General Object Perception (GOP) project for autonomous driving (2022.7.22).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by IJCV (2022.2).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by CVPR (2022.2).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by 3DV (2021.12).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by TCSVT (2021.5).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by ICLR (2021.3).
</ul>
<ul>
  <img alt="" height="30" src="./welcome_files/new.gif" width="30"/> One paper accepted by CVPR (2021.3).
</ul>


<h2>Competitions</h2>
<ul>
	<li>
		<font size="4"><b>No. 1</b> in DAVIS Challenge on Video Object Segmentation 2017. <a href="https://github.com/lxx1991/VS-ReID">[code]</a></font> <a href="https://liuziwei7.github.io/projects/VSReID.html">[project]</a>
	</li>
	<li>
		<font size="4"><b>No. 1</b> in ImageNet Object Detection Challenge 2016. <a href="./welcome_files/Poster_ImageNet_DeepID2.pdf">poster</a></font>
	</li>
	<li>
		<font size="4"><b>No. 1</b> in ImageNet Object Detection from Video Challenge 2015. <a href="./welcome_files/CUvideo_poster.pdf">poster</a> | <a href="./welcome_files/CUvideo_slides.pdf">slides</a></font>
	</li>
	<li>
		<font size="4"><b>No. 2</b> in ImageNet Object Detection Challenge 2015. <a href="./welcome_files/CUimage_poster.pdf">poster</a></font>
	</li>
	<li>
		<font size="4"><b>No. 2</b> in ImageNet Object Detection Challenge 2014. <a href="./welcome_files/CUHK_DeepID_Net_slides.ppsx">slides</a> </font>
	</li>

</ul>




<h2> Publications <a href="https://scholar.google.com.hk/citations?user=546GPMoAAAAJ&hl=zh-CN">[Google Scholar]</a></h2>

<ul>
  <li><p>Shaoshuai Shi, Chaoxu Guo, Li Jiang, <b>Zhe Wang</b>, Jianping Shi, Xiaogang Wang, Hongsheng Li,
  <a>"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection"</a>,  IEEE Conference on Computer Vision and Pattern Recognition, (CVPR), 2020 <a href="https://github.com/sshaoshuai/PV-RCNN">[code]</a></p> 
  <font color="red"> Ranks 1st on KITTI 3D object detection benchmark on Jan. 9th, 2020</font> <br /> 
</li>
</ul>


<ul>
  <li><p>Mingyu Ding, Yuqi Huo, Hongwei Yi, <b>Zhe Wang</b>, Jianping Shi, Zhiwu Lu, Ping Luo,
  <a>"Learning Depth-Guided Convolutions for Monocular 3D Object Detection"</a>,  IEEE Conference on Computer Vision and Pattern Recognition, (CVPR), 2020
  </p>
  <font color="red"> Ranks 1st on KITTI monocular 3D object detection benchmark on Nov. 11th, 2019</font> <br />
  </li>
</ul>


<ul>
  <li><p>Shaoshuai Shi, <b>Zhe Wang</b>, Xiaogang Wang, Hongsheng Li,
  <a>"From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network"</a>,  IEEE Transactions on Pattern Analysis and Machine Intelligence, (TPAMI), 2020 <a href="https://github.com/sshaoshuai/PCDet">[code]</a></p>
  <font color="red"> Ranks 1st on KITTI 3D object detection benchmark on July. 9th, 2019</font> <br />  
</li>
</ul>



<!-- This is a comment
<ul>
  <li><p>Hongwei Yi, Shaoshuai Shi, Mingyu Ding, Jiankai Sun, Hui Zhou, <b>Zhe Wang</b>, Sheng Li, Guoping Wang,
  <a>"SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud"</a>,  International Conference on Robotics and Automation (ICRA), 2020
  </p>
  </li>
</ul>

<ul>
  <li><p>Mingyu Ding, <b>Zhe Wang</b>, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo,
  <a>"Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow"</a>,  Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020
  </p>
  </li>
</ul>


<ul>
<li><p>Mingyu Ding, <b>Zhe Wang</b>, Zhiwu Lu,
<a>"Iteratively Optimized Semantic Autoencoder for Transductive Zero-shot Learning"</a>,  Computer Vision and Image Understanding (CVIU), 2019
</p>
</li>
</ul>-->

<ul>
<li><p>Wenwei Zhang, Hui Zhou, Shuyang Sun, <b>Zhe Wang</b>, Jianping Shi, Chen Change Loy,
<a>"Robust Multi Modality Multi-Object Tracking"</a>,  International Conference in Computer Vision (ICCV), 2019
</p>
</li>
</ul>


<ul>
<li><p>Mingyu Ding, <b>Zhe Wang</b>, Jiankai Sun, Jianping Shi, Ping Luo,
<a>"CamNet: Coarse-to-Fine Retrival for Camera Relocalization"</a>,  International Conference in Computer Vision (ICCV), 2019 <a href="https://github.com/dingmyu/CamNet">[code]</a>
</p>
</li>
</ul>


<!-- This is a comment
<ul>
<li><p>Yunhe Gao, Rui Huang, Ming Chen, <b>Zhe Wang</b>, Jincheng Deng, Yuanyuan Chen, Jie Zhang, Chanjuan Tao, and Hongsheng Li,
<a>"FocusNet: Imbalanced Large and Small Organ Segmentation with an End-to-End Deep Neural Network for Head and Neck CT Images"</a>,  International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2019
</p>
</li>
</ul>


<ul>
<li><p>Kui Xu, <b>Zhe Wang</b>, Jianping Shi, Hongsheng Li, Qiangfeng Cliff Zhang,
<a href="./welcome_files/papers/aaai19-xk.pdf">"A^2-Net: Molecular Structure Estimation from Cryo-EM Density Volumes"</a>, The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19). <i><b>(spotlight)</b></i>
</p>
</li>
</ul>

<ul>
<li><p>Fei Li, <b>Zhe Wang</b>, Guoxiang Qu, Diping Song, Ye Yuan, Yang Xu, Kai Gao, Guangwei Luo, Zegu Xiao, Dennis SC Lam, Hua Zhong, Yu Qiao, Xiulan Zhang
<a href="https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-018-0273-5">"Automatic differentiation of Glaucoma visual field from non-glaucoma visual filed using deep convolutional neural network"</a>, BMC medical imaging.
</p>
</li>
</ul>


<ul>
<li><p>G. Qu*, W. Zhang*, <b>Z. Wang*</b>, X. Dai, J. Shi, J. He, F. Lei, X. Zhang, Y. Qiao,
<a href="./welcome_files/papers/stripnet-topology-consistent_1xz.pdf">"StripNet: Towards Topology Consistent Strip Structure Segmentation"</a>, ACM Multimedia Conference (ACM-MM), 2018. <i>(*equal contribution)</i>
</p>
</li>
</ul>


<ul>
<li><p>C. Yang, <b>Z. Wang</b>, X. Zhu, C. Huang, J. Shi, D. Lin,
<a href="./welcome_files/papers/pose_guided.pdf">"Pose Guided Human Video Generation"</a>, European Conference on Computer Vision (ECCV), 2018. 
</p>
</li>
</ul>
-->


<ul>
<li><p>X. Li, Y. Qi, <b>Z. Wang</b>, K. Chen, Z. Liu, J. Shi, P. Luo, C. Change Loy, X. Tang,
<a href="https://arxiv.org/abs/1708.00197">"Video Object Segmentation with Re-identification"</a>, The 2017 DAVIS Challenge on Video Object Segmentation - CVPR Workshops (<b>1st place</b>), 2017. 
</p>
</li>
</ul>


<ul>
<li><p><b>Zhe Wang</b>, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li, Xiaogang Wang,
<a href="https://arxiv.org/abs/1706.04372">"Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection"</a>, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2017. 
</p>
</li>
</ul>


<ul>
<li><p>K. Kang, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, <b>Z. Wang</b>, R. Wang, X. Wang, W. Ouyang. 
<a href="./welcome_files/papers/kangLYZ_tubelets.pdf">"T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos"</a>, IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017.
</p>
</li>
</ul>

<ul>
<li><p>W. Ouyang, X. Zeng, X. Wang, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang, <b>Zhe Wang</b>, et al.
<a href="./welcome_files/papers/ouyangZWpami16.pdf">"DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks"</a>, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.
</p>
</ul>


<ul>
<li><p>X. Zeng, W. Ouyang, J. Yan, H. Li, T. Xiao, K. Wang, Y. Liu, Y. Zhou, B. Yang, <b>Zhe Wang</b>, H. Zhou, X. Wang. 
<a href="./welcome_files/papers/xyzeng_window-object.pdf">"Crafting GBD-Net for Object Detection"</a>, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017. 
</p>
</li>
</ul>

<ul>
<li><p><b>Zhe Wang</b>, H. Li, W. Ouyang, X. Wang.
<a href="./welcome_files/papers/learnable_histogram.pdf">"Learnable Histogram: Statistical Context Features for Deep Neural Networks"</a>, European Conference on Computer Vision (ECCV), 2016. <a href="./welcome_files/P-1A-15.pdf">(poster)</a>
</p></li>
</ul>

<!-- This is a comment
<ul>
<li><p><b>Zhe Wang</b>, H. Li, Q. Zhang, J. Yuan, X. Wang.
<a href="./welcome_files/papers/CSMRFML.pdf">"Magnetic Resonance Fingerprinting with Compressed Sensing and Distance Metric Learning"</a>, Journal of Neurocomputing, 2016.
</p>
</li>
</ul>
-->

<ul>
<li><p>W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang, <b>Zhe Wang</b>, C. Loy, X. Tang.
<a href="./welcome_files/papers/deepidcvpr.pdf">"DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection"</a>, In Proc. CVPR 2015.
</p>
<p><a href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNet/index.html">Models & Code</a></p></li>
</ul>


<!-- This is a comment
<ul>
<li><p>W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang, <b>Zhe Wang</b>, C. Loy, X. Tang.
<a href="./welcome_files/papers/deepidarxiv.pdf">"Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection"</a>, Preprint, arXiv:1409.3505.
</p>
</ul>
-->


<h2>  Professional Activities </h2>
Journal Reviewer
<ul>
  <li>
  <p>TCSVT, TBME, MIA, etc</p>
  </li>
</ul>

Conference Reviewer
<ul>
  <li>
  <p><dot>CVPR 2018~2020, ICCV 2019~2021, ECCV 2020, MICCAI 2018~2019, etc.</p>
  </li>
</ul>

<!-- This is a comment
     </ul>

<h2> Teaching Experience </h2>


<ul>
<li>ENGG5202</li> Pattern Recognition
</ul>
<ul>
<li>ENGG1110</li> Problem Solving by Programming
</ul>
<ul>
<li>BMEG4320</li> Biomedical Imaging Applications
</ul>
<ul>
<li>ELEG2202</li> Circuit and Device
</ul>
<ul>
<li>ENGG1110B</li> Basic Circuit Theory
</ul> -->

<br>
<div><a href="http://info.flagcounter.com/Gjat"><img src="http://s10.flagcounter.com/count/Gjat/bg_FFFFFF/txt_000000/border_CCCCCC/columns_3/maxflags_12/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0" align="middle"></a>
</div>

<div id="footer">
<div id="footer-text">


</div>

</div>
</div>
</body></html>

